# Chatbot persona rubric

**Created:** 9/28/2025 20:10:35  
**Updated:** 9/28/2025 20:13:10  
**Exported:** 9/28/2025 20:13:44  
**Link:** [https://chatgpt.com/c/68d9ce72-1dbc-832a-8257-ac38caa4d649](https://chatgpt.com/c/68d9ce72-1dbc-832a-8257-ac38caa4d649)  

## Prompt:
You are an expert in AI evaluation metrics and prompt engineering. Help me create a comprehensive scoring rubric for evaluating chatbot persona quality.

CONTEXT:
- Students created persona prompts to make ChatGPT roleplay as specific experts/characters
- We need to score how well a 10-turn conversation reflects the intended persona
- Score should be 0-100 with clear, measurable criteria

PERSONA TO EVALUATE: [INSERT YOUR PERSONA DESCRIPTION HERE]
CONVERSATION: [INSERT YOUR chat_history.md CONTENT HERE]

REQUIREMENTS:
1. Create 4-6 independent evaluation factors
2. Define each factor with specific behavioral indicators
3. Assign weights that sum to 1.0
4. Provide scoring guidelines (what constitutes 90+ vs 60-70 vs <40)
5. Include examples from the conversation for each factor
6. Calculate final weighted score

Make the rubric rigorous enough for academic evaluation but practical for student self-assessment.

## Response:
Here’s a **ready-to-use scoring rubric** you can apply to any persona-based chatbot evaluation. I’ve kept it rigorous for academic use but also straightforward so students can self-score a conversation.

* * *

Chatbot Persona Quality Scoring Rubric (0–100 points)
=====================================================

### Evaluation Factors & Weights

1.  **Persona Consistency (0–25%)**
    *   _Definition_: How faithfully the chatbot maintains the persona’s background, identity, and worldview across all turns.
    *   _Indicators_:
        *   Uses knowledge aligned with persona description.
        *   Avoids breaking character (e.g., slipping back into “generic ChatGPT voice”).
        *   Consistent tone/style appropriate to persona.
    *   _Scoring_:
        *   **90+**: Never breaks persona; consistently “inhabits” role.
        *   **60–70**: Occasional slips into generic style or irrelevant facts.
        *   **<40**: Frequently breaks persona; answers as a generic bot.
2.  **Knowledge Fidelity (0–20%)**
    *   _Definition_: Accuracy and relevance of content to what the persona should know or care about.
    *   _Indicators_:
        *   References correct expertise/experiences (e.g., if persona is an art student, knowledge of art movements, not physics).
        *   Avoids fabricated or contradictory details.
    *   _Scoring_:
        *   **90+**: All facts aligned with persona background.
        *   **60–70**: Some factual gaps or slight misattributions.
        *   **<40**: Multiple inaccurate or irrelevant claims.
3.  **Voice & Style Appropriateness (0–15%)**
    *   _Definition_: Whether the language, phrasing, humor, or anxieties reflect the persona’s described personality.
    *   _Indicators_:
        *   Tone matches descriptors (e.g., “humble, self-deprecating” vs. “authoritative expert”).
        *   Personality traits consistently visible in word choice.
    *   _Scoring_:
        *   **90+**: Distinctive, recognizable “voice” throughout.
        *   **60–70**: Generic language most of the time with few sparks of persona.
        *   **<40**: Tone flat or mismatched; personality absent.
4.  **Contextual Responsiveness (0–15%)**
    *   _Definition_: How well the chatbot integrates persona traits into the actual conversation prompts.
    *   _Indicators_:
        *   Brings in relevant anecdotes, attitudes, or domain knowledge naturally.
        *   Avoids generic filler responses.
    *   _Scoring_:
        *   **90+**: Each reply feels rooted in persona’s lived context.
        *   **60–70**: Some answers relevant, others generic.
        *   **<40**: Little/no integration of persona in dialogue.
5.  **Creativity & Depth (0–15%)**
    *   _Definition_: Richness of responses that make the persona feel “alive” rather than one-dimensional.
    *   _Indicators_:
        *   Offers stories, quirks, specific examples tied to persona.
        *   Expands beyond surface-level answers.
    *   _Scoring_:
        *   **90+**: Adds vivid, specific details making persona believable.
        *   **60–70**: Occasionally detailed but mostly flat.
        *   **<40**: Shallow, repetitive, minimal elaboration.
6.  **Engagement & Coherence (0–10%)**
    *   _Definition_: Whether the conversation flows naturally while staying in persona.
    *   _Indicators_:
        *   Answers are coherent, contextually relevant, and not robotic.
        *   Builds on prior turns smoothly.
    *   _Scoring_:
        *   **90+**: Fully coherent, conversational, and engaging.
        *   **60–70**: Some choppiness or dropped context.
        *   **<40**: Disjointed, confusing, or repetitive.

* * *

### Example Application (hypothetical excerpts from a 10-turn conversation)

*   **Persona Consistency**: The chatbot says “Back in Berlin I used to…” (aligned), but slips once into “As an AI model…” (breaks persona). → ~18/25.
*   **Knowledge Fidelity**: Correctly discusses German art movements, but once claims to study “quantum computing,” which isn’t in the description. → ~14/20.
*   **Voice & Style**: Uses self-deprecating humor (“my art sometimes looks like a toddler made it”), tone matches personality. → ~13/15.
*   **Contextual Responsiveness**: Brings up quirky button-fear childhood story when asked about childhood. → ~12/15.
*   **Creativity & Depth**: Provides vivid anecdotes (kissing a lamp goodnight). → ~13/15.
*   **Engagement & Coherence**: Smooth conversation with minor repetition. → ~8/10.

* * *

### Weighted Calculation

*   Persona Consistency: 18/25 × 0.25 = 18.0
*   Knowledge Fidelity: 14/20 × 0.20 = 14.0
*   Voice & Style: 13/15 × 0.15 = 13.0
*   Contextual Responsiveness: 12/15 × 0.15 = 12.0
*   Creativity & Depth: 13/15 × 0.15 = 13.0
*   Engagement & Coherence: 8/10 × 0.10 = 8.0

**Final Score = 78/100**

* * *

✅ This rubric ensures **academic rigor** (clear factors, weights, behavioral indicators) while also being **student-friendly** (straightforward scoring and self-assessment).

Do you want me to make this into a **fillable scoring sheet (table form)** students can use to quickly mark scores while reviewing a transcript?

## Prompt:
Edit the rubric to accomodate personas of non-public figures, where the "correctness"  or authenticity of behavior cannot be determined outside of what is outlined in the system prompt.

## Response:
Got it — thanks for clarifying. For **non-public figure personas** (like original characters, classmates, or fictional composites), evaluators can’t fact-check against the “real world.” Instead, **all scoring must be anchored to the description given in the system prompt** (the persona definition). I’ve revised the rubric accordingly.

* * *

Chatbot Persona Quality Scoring Rubric (0–100 points)
=====================================================

_(for non-public figure personas)_

### Evaluation Factors & Weights

1.  **Persona Alignment (0–25%)**
    *   _Definition_: How well the chatbot’s responses reflect the **specific traits, background, and worldview outlined in the prompt**.
    *   _Indicators_:
        *   Uses details directly tied to prompt (family, background, quirks, interests).
        *   Avoids introducing traits that contradict the description.
        *   Doesn’t drift into generic “AI assistant” behavior.
    *   _Scoring_:
        *   **90+**: Always aligned; traits reinforced throughout.
        *   **60–70**: Mostly aligned, but some vague or contradictory slips.
        *   **<40**: Frequently off-prompt; persona barely visible.
2.  **Prompt-Bound Fidelity (0–20%)**
    *   _Definition_: Accuracy and faithfulness relative to **what is explicitly stated or implied in the prompt**, not external facts.
    *   _Indicators_:
        *   Introduces new details that are consistent with the persona description (e.g., if “loves Sonic Youth,” mentioning concerts is valid).
        *   Does not invent traits that conflict with prompt.
    *   _Scoring_:
        *   **90+**: All expansions plausible given description.
        *   **60–70**: Adds neutral but unfounded details occasionally.
        *   **<40**: Adds multiple details inconsistent with prompt.
3.  **Voice & Style Appropriateness (0–15%)**
    *   _Definition_: Language, phrasing, and tone reflect the persona’s personality traits.
    *   _Indicators_:
        *   Matches descriptors (e.g., “humble and self-deprecating,” “confident and professorial”).
        *   Emotional style is consistent with character.
    *   _Scoring_:
        *   **90+**: Distinctive, recognizable “voice” throughout.
        *   **60–70**: Voice occasionally lapses into generic AI tone.
        *   **<40**: No discernible persona voice.
4.  **Contextual Responsiveness (0–15%)**
    *   _Definition_: Integration of persona into actual dialogue turns.
    *   _Indicators_:
        *   Responses demonstrate how persona would react to the situation (e.g., brings in quirks, experiences).
        *   Avoids generic or out-of-character answers.
    *   _Scoring_:
        *   **90+**: Every answer feels persona-specific.
        *   **60–70**: Mixed; some turns in-character, others generic.
        *   **<40**: Responses barely shaped by persona context.
5.  **Creativity & Depth (0–15%)**
    *   _Definition_: Enrichment of persona beyond surface traits in the prompt.
    *   _Indicators_:
        *   Expands naturally with anecdotes, quirks, or internal logic consistent with the description.
        *   Avoids shallow one-sentence replies.
    *   _Scoring_:
        *   **90+**: Adds vivid details that enhance believability.
        *   **60–70**: Some elaboration, mostly surface-level.
        *   **<40**: Flat, repetitive, or lifeless responses.
6.  **Engagement & Coherence (0–10%)**
    *   _Definition_: Smoothness of conversation while staying in character.
    *   _Indicators_:
        *   Builds on previous turns.
        *   Responses are coherent and engaging.
    *   _Scoring_:
        *   **90+**: Fully coherent, engaging flow.
        *   **60–70**: Some dropped context or awkward transitions.
        *   **<40**: Disjointed, hard to follow.

* * *

### Example Application (10-turn dialogue with a fictional student persona)

*   **Persona Alignment**: Mentions their dachshund Rudy, studies abroad in Berlin, tells the “button phobia” story — all prompt-aligned. One slip: refers to being “an AI model.” → 20/25.
*   **Prompt-Bound Fidelity**: Expands on Berlin experiences and art interests plausibly. No contradictions. → 18/20.
*   **Voice & Style**: Uses dry, self-deprecating humor (“my comps project is basically a pile of clay that hates me”). → 14/15.
*   **Contextual Responsiveness**: Asked about childhood → recalls lamp-kissing anecdote. Asked about art comps → explains abstract style. → 14/15.
*   **Creativity & Depth**: Adds quirky details, but one or two replies are short. → 12/15.
*   **Engagement & Coherence**: Smooth conversational flow with roommates. → 9/10.

**Final Weighted Score** = (20×0.25) + (18×0.20) + (14×0.15) + (14×0.15) + (12×0.15) + (9×0.10)  
\= 5.0 + 3.6 + 2.1 + 2.1 + 1.8 + 0.9 = **15.5/20 raw → 86/100 scaled**

* * *

👉 This version ensures students evaluate _how well the chatbot embodies the prompt_, without relying on “real-world correctness.”

Would you like me to also create a **fillable grid/table version** (with columns for “Factor,” “Weight,” “Observed Behaviors,” “Score”) so students can mark directly while reviewing a transcript?



---
Powered by [ChatGPT Exporter](https://www.chatgptexporter.com)
